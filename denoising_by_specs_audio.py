# -*- coding: utf-8 -*-
"""denoising_by_specs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Bf2vTfdc9fD1oPoGbQHPIEXEBC8qpSh

# NOTES

this notebook is made so that we can use this project as we use it in command line on windows.
this project was uploaded by sarfaraz on 1 october.

we started working on this project from 25th september. roughly a week before from today (2 october 2022 monday). its sunday i am oficce. this day.

yeh project by default, 8000 sampling rate par kaam kar raha hai.

live audio record karne wala kaam kardia. par woh 48khz par record kar raha hai. i think agar 8000 par kare to results ziada behtar aenge. coz hamara model 8khz ki audios par train hoa hai.

**findings:**

the denoising is working great when there is silences between the speeches. but when person speaks the noise slips with its voice and its hearable.

on the dataset the model is working fine, the above thing was on real world data when i tested the model on audio clips from a tour of noisy data centre.

when i trained the model on colab , it trained until 13 epochs on batch size 5 with 12000 samples. and than crashed. i think model did not got saved only on one epoch it got saved. so if we retrain the model on a huge dataset might it produces great results on real world data.

"""


import librosa.display
import matplotlib.pyplot as plt
import pydub
from keras.models import load_model
from tensorflow.keras.models import model_from_json
import scipy.io.wavfile
import tensorflow as tf
import librosa
import soundfile as sf
import numpy as np
import subprocess
import os


def audio_to_audio_frame_stack(sound_data, frame_length, hop_length_frame):
    """This function take an audio and split into several frame
       in a numpy matrix of size (nb_frame,frame_length)"""

    sequence_sample_length = sound_data.shape[0]

    sound_data_list = [sound_data[start:start + frame_length] for start in range(
        0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows
    sound_data_array = np.vstack(sound_data_list)

    return sound_data_array


def audio_files_to_numpy(audio_dir, list_audio_files, sample_rate, frame_length, hop_length_frame, min_duration):
    """This function take audio files of a directory and merge them
    in a numpy matrix of size (nb_frame,frame_length) for a sliding window of size hop_length_frame"""

    list_sound_array = []

    for file in list_audio_files:
        # open the audio file
        y, sr = librosa.load(os.path.join(audio_dir, file), sr=sample_rate)
        # this piece of code is padding the audio so that i can be read till the end.
        lenght_noisy = len(y)
        while lenght_noisy % 8064 != 0:
            lenght_noisy = lenght_noisy + 1

        needed_padding_zero = lenght_noisy - len(y)
        zeros = [0] * needed_padding_zero
        print('needed zeros -- >', len(zeros))
        y = np.concatenate((y, zeros), axis=0)
        # now we have a final audio which will denoise fully.

        total_duration = librosa.get_duration(y=y, sr=sr)

        if (total_duration >= min_duration):
            list_sound_array.append(audio_to_audio_frame_stack(
                y, frame_length, hop_length_frame))
        else:
            print(
                f"The following file {os.path.join(audio_dir,file)} is below the min duration")

    return np.vstack(list_sound_array)


def blend_noise_randomly(voice, noise, nb_samples, frame_length):
    """This function takes as input numpy arrays representing frames
    of voice sounds, noise sounds and the number of frames to be created
    and return numpy arrays with voice randomly blend with noise"""

    prod_voice = np.zeros((nb_samples, frame_length))
    prod_noise = np.zeros((nb_samples, frame_length))
    prod_noisy_voice = np.zeros((nb_samples, frame_length))

    for i in range(nb_samples):
        id_voice = np.random.randint(0, voice.shape[0])
        id_noise = np.random.randint(0, noise.shape[0])
        level_noise = np.random.uniform(0.2, 0.8)
        prod_voice[i, :] = voice[id_voice, :]
        prod_noise[i, :] = level_noise * noise[id_noise, :]
        prod_noisy_voice[i, :] = prod_voice[i, :] + prod_noise[i, :]

    return prod_voice, prod_noise, prod_noisy_voice


def audio_to_magnitude_db_and_phase(n_fft, hop_length_fft, audio):
    """This function takes an audio and convert into spectrogram,
       it returns the magnitude in dB and the phase"""

    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)
    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)

    stftaudio_magnitude_db = librosa.amplitude_to_db(
        stftaudio_magnitude, ref=np.max)

    return stftaudio_magnitude_db, stftaudio_phase


def numpy_audio_to_matrix_spectrogram(numpy_audio, dim_square_spec, n_fft, hop_length_fft):
    """This function takes as input a numpy audi of size (nb_frame,frame_length), and return
    a numpy containing the matrix spectrogram for amplitude in dB and phase. It will have the size
    (nb_frame,dim_square_spec,dim_square_spec)"""

    nb_audio = numpy_audio.shape[0]

    m_mag_db = np.zeros((nb_audio, dim_square_spec, dim_square_spec))
    m_phase = np.zeros(
        (nb_audio, dim_square_spec, dim_square_spec), dtype=complex)

    for i in range(nb_audio):
        m_mag_db[i, :, :], m_phase[i, :, :] = audio_to_magnitude_db_and_phase(
            n_fft, hop_length_fft, numpy_audio[i])

    return m_mag_db, m_phase


def magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, stftaudio_magnitude_db, stftaudio_phase):
    """This functions reverts a spectrogram to an audio"""

    stftaudio_magnitude_rev = librosa.db_to_amplitude(
        stftaudio_magnitude_db, ref=1.0)

    # taking magnitude and phase of audio
    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase
    audio_reconstruct = librosa.core.istft(
        audio_reverse_stft, hop_length=hop_length_fft, length=frame_length)

    return audio_reconstruct


def matrix_spectrogram_to_numpy_audio(m_mag_db, m_phase, frame_length, hop_length_fft):
    """This functions reverts the matrix spectrograms to numpy audio"""

    list_audio = []

    nb_spec = m_mag_db.shape[0]

    for i in range(nb_spec):

        audio_reconstruct = magnitude_db_and_phase_to_audio(
            frame_length, hop_length_fft, m_mag_db[i], m_phase[i])
        list_audio.append(audio_reconstruct)

    return np.vstack(list_audio)


def scaled_in(matrix_spec):
    "global scaling apply to noisy voice spectrograms (scale between -1 and 1)"
    matrix_spec = (matrix_spec + 46)/50
    return matrix_spec


def scaled_ou(matrix_spec):
    "global scaling apply to noise models spectrograms (scale between -1 and 1)"
    matrix_spec = (matrix_spec - 6)/82
    return matrix_spec


def inv_scaled_in(matrix_spec):
    "inverse global scaling apply to noisy voices spectrograms"
    matrix_spec = matrix_spec * 50 - 46
    return matrix_spec


def inv_scaled_ou(matrix_spec):
    "inverse global scaling apply to noise models spectrograms"
    matrix_spec = matrix_spec * 82 + 6
    return matrix_spec


#  prediction file oper wali data tool files thi.

# from data_tools import scaled_in, inv_scaled_ou
# from data_tools import audio_files_to_numpy, numpy_audio_to_matrix_spectrogram, matrix_spectrogram_to_numpy_audio

# Load previously saved model

# function of moving point average used for minimizing distortion in denoised audio.

def moving_average(x, w):
    return np.convolve(x, np.ones(w), 'valid') / w


def prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,
               audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft):
    """ This function takes as input pretrained weights, noisy voice sound to denoise, predict
    the denoise sound and save it to disk.
    """
    print("weights path -- >", weights_path)
    print("name of model -->", name_model)
    print("audio dir prediction--> ", audio_dir_prediction)
    print("dir save prediction -->", dir_save_prediction)
    print("audio input for pred --> ", audio_input_prediction)
    print("audio output pred -- >", audio_output_prediction)
    print("sampling rate --> ", sample_rate)
    print("min duration -->", min_duration)
    print("frame lenght -->", frame_length)
    print("hop lenght frame --> ", hop_length_frame)
    print("n ftt ->", n_fft)
    print("hop lenght fft -->", hop_length_fft)
    # load json and create model
    # json_file = open(weights_path+'/'+name_model+'.json', 'r')
    # loaded_model_json = json_file.read()
    # json_file.close()
    # loaded_model = model_from_json(loaded_model_json)

    # load weights into new model
    # loaded_model.load_weights(weights_path+'/'+name_model+'.h5')

    # Load previously saved model
    model_loc = weights_path+'/'+name_model+'.h5'
    loaded_model = load_model(model_loc, compile=False)
    print(model_loc)

    print("Loaded model from disk")

    print(audio_input_prediction)
    print(audio_dir_prediction)

    # Extracting noise and voice from folder and convert to numpy
    audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,
                                 frame_length, hop_length_frame, min_duration)

    # Dimensions of squared spectrogram
    dim_square_spec = int(n_fft / 2) + 1
    print(dim_square_spec)

    # Create Amplitude and phase of the sounds
    m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(
        audio, dim_square_spec, n_fft, hop_length_fft)

    # global scaling to have distribution -1/1
    X_in = scaled_in(m_amp_db_audio)
    # Reshape for prediction
    X_in = X_in.reshape(X_in.shape[0], X_in.shape[1], X_in.shape[2], 1)
    # Prediction using loaded network
    X_pred = loaded_model.predict(X_in)
    # Rescale back the noise model
    inv_sca_X_pred = inv_scaled_ou(X_pred)
    # Remove noise model from noisy speech
    # print("m_amp_db_audio --> ", m_amp_db_audio)
    X_denoise = (m_amp_db_audio) - (inv_sca_X_pred[:, :, :, 0] * 2)
    X_denoise = X_denoise * 2
    # X_denoise = X_denoise + 2
    # Reconstruct audio from denoised spectrogram and phase
    print(X_denoise.shape)
    print(m_pha_audio.shape)
    print(frame_length)
    print(hop_length_fft)

    # for reconstructing the denoised audio.
    audio_denoise_recons = matrix_spectrogram_to_numpy_audio(
        X_denoise, m_pha_audio, frame_length, hop_length_fft)
    # Number of frames
    nb_samples = audio_denoise_recons.shape[0]
    # Save all frames in one file
    denoise_long = audio_denoise_recons.reshape(
        1, nb_samples * frame_length)*10
    # librosa.output.write_wav(dir_save_prediction + audio_output_prediction, denoise_long[0, :], sample_rate)
    # here i have inserted moving point average to see if the audio vocal quality gets better or not.
    denoise_long_mov_avg = moving_average(denoise_long[0, :], 4)
    # previous way of saving the file
    # scipy.io.wavfile.write(
    #     dir_save_prediction + audio_output_prediction, sample_rate, denoise_long_mov_avg)
    # new way of saving the file , this maybe solves the header problem.
    sf.write(dir_save_prediction + audio_output_prediction, denoise_long_mov_avg.astype(
        np.float32), 8000, 'PCM_16')

    # for noise that is predicted,  i also want to hear it.
    audio_denoise_noise_recons = matrix_spectrogram_to_numpy_audio(
        inv_sca_X_pred[:, :, :, 0], m_pha_audio, frame_length, hop_length_fft)
    # Number of frames
    nb_samples_noise = audio_denoise_noise_recons.shape[0]
    # Save all frames in one file
    denoise_long_pred_noise = audio_denoise_noise_recons.reshape(
        1, nb_samples_noise * frame_length)*10
    denoise_long_pred_noise_mov_avg = moving_average(
        denoise_long_pred_noise[0, :], 4)
    scipy.io.wavfile.write(r'data/Test/sound/predicted_noise/pred_noise.wav',
                           sample_rate, denoise_long_pred_noise_mov_avg)
    # sf.write('C:/Users/sarfa/Downloads/FYP/server/tmp/predicted_noise.wav', predicted_noise.astype(
    # np.float32), 22050, 'PCM_16')

    # scipy.io.wavfile.write("../output/before_noisy.wav", 44100, audio_noisy_44100)


# denoising.
# passed arguments.
weights_path = r'./weights/'
name_of_model = r'model_best'
audio_dir_prediction = r'../client/src/denoised-audio'
dir_save_prediction = r'../client/src/denoised-audio/'
audio_input_for_pred = ['recording.wav']
audio_output_pred = r'denoised.wav'

sr = 8000
min_duration = 1.0
frame_length = 8064
# no overlap hop
#  806 = 10 percent overlap
# 403 =  5 percent overlap
# 201.5 = 2.5 percent overlap
# hop_length_frame = 8064 + 100
hop_length_frame = 8064
n_fft = 255
hop_length_fft = 63


prediction(weights_path, name_of_model, audio_dir_prediction, dir_save_prediction, audio_input_for_pred,
           audio_output_pred, sr, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft)

print("boosting audio")
wav_file = pydub.AudioSegment.from_file(
    file="../client/src/denoised-audio/denoised.wav", format="wav")
# Increase the volume by 10 dB
new_wav_file = wav_file + 10
# Reducing volume by 5
# samples = new_wav_file.get_array_of_samples()
# factor = 32767 / max(samples)
# print(factor)
silent_wav_file = wav_file - 5
# Feel the difference!
new_wav_file.export(
    out_f="../client/src/denoised-audio/denoised.wav", format="wav")

print("making spectrograms")

#  spectrogram generation for noisy input and denoised output
noisy_path = r'../client/src/denoised-audio/recording.wav'
denoised_path = r'../client/src/denoised-audio/denoised.wav'

noisy, sr_noisy = librosa.load(noisy_path)
denoised, sr_denoised = librosa.load(denoised_path)

S_noisy = librosa.feature.melspectrogram(
    y=noisy, sr=sr_noisy, n_mels=128, fmax=8000)

S_denoised = librosa.feature.melspectrogram(
    y=denoised, sr=sr_noisy, n_mels=128, fmax=8000)

# for noisy input audio.
fig, ax = plt.subplots()
S_dB_noisy = librosa.power_to_db(S_noisy, ref=np.max)
img_noisy = librosa.display.specshow(
    S_dB_noisy, x_axis='time', y_axis='mel', sr=sr_noisy, fmax=8000, ax=ax)
fig.colorbar(img_noisy, ax=ax, format='%+2.0f dB')
ax.set(title='Mel-frequency spectrogram noisy')

fig.savefig(r'../client/src/recorded-audio-spectograms/spec_noisy.png')

# for denoised spectrogram.
fig, ax = plt.subplots()
S_dB_denoised = librosa.power_to_db(S_denoised, ref=np.max)
img_denoised = librosa.display.specshow(
    S_dB_denoised, x_axis='time', y_axis='mel', sr=sr_denoised, fmax=8000, ax=ax)
fig.colorbar(img_denoised, ax=ax, format='%+2.0f dB')
ax.set(title='Mel-frequency spectrogram denoised')

fig.savefig(r'../client/src/recorded-audio-spectograms/spec_denoised.png')

print("success")
